{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Networks\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/en/7/7b/A_simple_Markov_network.png\" width=\"400px\" height=\"300px\" />\n",
    "\n",
    "> Until now, we have only studied **Bayesian Networks**, which are based on directed graphs. Among other features, these representations are nice because:\n",
    "> - Their structure often correspond to the causality of the phenomena they are modeling.\n",
    "> - Their parameters can be naturally elicited from the real-world phenomena.\n",
    "> \n",
    "> Now, we will study undirected graphical models. We will see that these models are useful when modeling certain situations where one cannot assign a directionality to the interaction between variables. In addition:\n",
    "> - The independence statements undirected models encode are often simpler to obtain than for the directed case.\n",
    "> - The inference tasks, which we will see the next module, are formulated independently of the direction of the edges.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - To describe the Markov networks representation, which is based in undirected graphs.\n",
    "> - To learn how to represent Markov networks, their independence properties and their semantics.\n",
    "> - To compare the the independencies that can be encoded into Markov networks to those that can be encoded in Bayesian networks.\n",
    "\n",
    "> **Referencias:**\n",
    "> \n",
    "> - Probabilistic Graphical Models: Principles and Techniques, By Daphne Koller and Nir Friedman. Ch. 4.\n",
    "> - Probabilistic Graphical Models Specialization, offered through Coursera. Prof. Daphne Koller.\n",
    "> - Mastering Probabilistic Graphical Models Using Python, By Ankur Ankan and Abinash Panda. Ch. 2.\n",
    "\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://upload.wikimedia.org/wikipedia/en/7/7b/A_simple_Markov_network.png.</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview\n",
    "\n",
    "As usual, let's introduce today's topic with an example.\n",
    "\n",
    "Let's assume that there are four people:\n",
    "- $A$(lex)\n",
    "- $B$(eatriz)\n",
    "- $C$(ristina)\n",
    "- $D$(aniel)\n",
    "\n",
    "We know that:\n",
    "- $A$ and $B$ are a couple,\n",
    "- $C$ and $D$ are a couple,\n",
    "- $A$ and $D$ are best friends,\n",
    "- $C$ and $B$ are best friends, however\n",
    "- $A$ and $C$ don't get along, and\n",
    "- $B$ and $D$ don't get along.\n",
    "\n",
    "So, these two couples decide to hang out some day. Once they get together, they have to decide where they want to go for dinner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our social expreience, we know that people who get along well can influence each other in their choices.\n",
    "\n",
    "In this situation, we have that $A$ influences $B$, $B$ influences $C$ so that, in the end $A$ indirectly influences $C$. However, given the choices of $B$ and $D$, the choice of $A$ shouldn't affect the choice of $C$ (and viceversa). In probabilistic words: $(A \\perp C | B, D)$.\n",
    "\n",
    "Similarly, given the choices of $A$ and $C$, the choice of $B$ shouldn't affect the choice of $D$ (and viceversa). In probabilistic words: $(B \\perp D | A, C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to model this situation using a Bayesian network, we may come up with the following graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/BNDecision.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- What independencies are encoded by the graph of the left?\n",
    "  - *Answer:* $(A \\perp C | B,D)$.\n",
    "  \n",
    "- What independencies are encoded by the graph of the right?\n",
    "  - *Answer:* $(B \\perp D | A, C)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, there is no Bayesian network which can encode these simple independencies $(A \\perp C | B, D)$ and $(B \\perp D | A, C)$, but no other.\n",
    "\n",
    "For a further discussion about this, please refer to Example 3.8 in the main book (page 83)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, we can easily implement these independencies using an *undirected graph* (*Markov network*).\n",
    "\n",
    "- As in Bayesian networks, the nodes of a Markov network (MN) represent random variables.\n",
    "\n",
    "- But different from Bayesian networks, we represent interaction between these random variables using undirected edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the latter has the direct implication that a node has not \"parents\" nor \"childs\", then we cannot represent the relations via the CPDs\n",
    "\n",
    "$$P(X_i | \\mathrm{Pa}X_i),$$\n",
    "\n",
    "but with general factors.\n",
    "\n",
    "Recall the definition of factor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Definition.* A **factor** $\\phi(X_1, \\dots, X_k)$ is a function whose arguments are values of the RVs $X_1, \\dots, X_k$ and returns a nonnegative real number:\n",
    "> \n",
    "> $$\\phi: \\mathrm{Val}(X_1, \\dots, X_k) \\to \\mathbb{R}_{\\geq 0}.$$\n",
    ">\n",
    "> The set of RVs $\\bar{D} = \\{X_1, \\dots, X_k\\}$ is called the **scope** of the factor $\\phi$, and this is denoted as $\\mathrm{scope}[\\phi]=\\bar{D}\\{X_1, \\dots, X_k\\}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we are ready to consider a model that encodes the mentioned independencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/MNFirst.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, are several things to note about the factors:\n",
    "\n",
    "- The factors are not probabilities. Then, they do not need to be normalized.\n",
    "- The factors are assigned to the edges **(they do not need to be assigned to only one edge though)**, and (possibly) to the nodes.\n",
    "- The factors represent, in a general sense, affinity functions between the variables that are in the scope of the factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, assume that in our \"couples example\" the restaurant options have been reduced to two:\n",
    "- Restaurant 0.\n",
    "- Restaurant 1.\n",
    "\n",
    "Then, looking for example at $\\phi_1$, one can conclude that:\n",
    "- $A$ and $B$ tend to agree on their restaurant selection.\n",
    "- When $A$ and $B$ disagree, there is more weight for $A$ choosing Restaurant 1 and $B$ choosing Restaurant 0 than the converse.\n",
    "\n",
    "We can come up with similar conclusions for the affinity between $B$ and $C$, $C$ and $D$, and $D$ and $A$.\n",
    "\n",
    "For example, $\\phi_3$ tells us that $C$ and $D$ strongly tend to disagree on their restaurant selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, similar to BNs, the factors in MN ecode local interactions between the variables in their scope.\n",
    "\n",
    "On the other hand, to define the global model (the joint distribution), we use the notion of product of factors:\n",
    "\n",
    "$$\\tilde{P}(A,B,C,D) = \\phi_1(A, B) \\cdot \\phi_2(B, C) \\cdot \\phi_3(C, D) \\cdot \\phi_4(D, A).$$\n",
    "\n",
    "However note that $\\tilde{P}$ is not a probability distribution, since it is unnormalized. The joint distribution is obtained normalizing $\\tilde{P}$:\n",
    "\n",
    "$$P(A,B,C,D) = \\frac{1}{Z}\\tilde{P}(A,B,C,D) = \\frac{1}{Z}\\phi_1(A, B) \\cdot \\phi_2(B, C) \\cdot \\phi_3(C, D) \\cdot \\phi_4(D, A),$$\n",
    "\n",
    "where\n",
    "\n",
    "$$Z = \\sum_{A,B,C,D} \\tilde{P}(A,B,C,D) = \\sum_{A,B,C,D} \\phi_1(A, B) \\cdot \\phi_2(B, C) \\cdot \\phi_3(C, D) \\cdot \\phi_4(D, A)$$\n",
    "\n",
    "is a normalizing constant known as the **partition function**. The term *function* is not clear by now, because $Z$ is a constant; however, it will be clear in the learning module, because $Z$ is indeed a function of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to define the above factors and the probability distribution using `pgmpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pgmpy.factors.discrete.DiscreteFactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define factors phi_i (i=1,2,3,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print any factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unnormalized measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unnormalized measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print partition function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print joint prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Knowing all the above, the pairwise factor $\\phi_1(A,B)$ is proportional to:\n",
    "\n",
    "1. The marginal probability $P(A,B)$.\n",
    "2. The conditional probability $P(A|B)$.\n",
    "3. The conditional probability $P(A, B | C, D)$.\n",
    "4. None of the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal probability P(A, B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional probability P(A | B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From these examples, we can note that the marginal probabilities (such as $P(A,B)$) are complicated aggregates of the factors that define the MN.\n",
    "\n",
    "- There is not a natural mapping between the probability distribution and the factors that are used to compose it. Think about the simple example above:\n",
    "  \n",
    "  $$P(A,B,C,D) = \\frac{1}{Z}\\phi_1(A, B) \\cdot \\phi_2(B, C) \\cdot \\phi_3(C, D) \\cdot \\phi_4(D, A).$$\n",
    "  \n",
    "- This, indeed, severely affects the learning procedures we will study in the third module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 2. Gibbs distributions and Markov networks\n",
    "\n",
    "The \"couples example\" we considered above, is a special case of Markov networks:\n",
    "\n",
    "> *Definition.* A **pairwise Markov network** is an undirected graph with nodes $X_1, \\dots, X_n$ and each edge $X_i - X_j$ is associated with a factor $\\phi_{ij}(X_i, X_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this class of MNs are easy to understand, since the interactions are encoded by pairs of RVs, they can only represent a very reduced class of joint distributions.\n",
    "\n",
    "Why?\n",
    "\n",
    "- Let's assume that have a joint distribution over $X_1,\\dots, X_n$.\n",
    "- Moreover, assume that $|\\mathrm{Val}(X_i)| = d$ for all $i=1,\\dots,n$.\n",
    "- How many parameters does one need to completely specify the whole distribution without any independence assumption?\n",
    "  \n",
    "  $$d^n$$\n",
    "  \n",
    "- Now, in a pairwise markov network we have $\\left(\\begin{array}{c}n \\\\ 2\\end{array}\\right)$ edges.\n",
    "- We need $d^2$ parameters to specify each factor.\n",
    "- In total,\n",
    "  $$\\left(\\begin{array}{c}n \\\\ 2\\end{array}\\right)d^2 \\leq (e/2)^2 n^2 d^2$$\n",
    "  \n",
    "Inequality from: https://www.johndcook.com/blog/2008/11/10/bounds-on-binomial-coefficients/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, **we cannot encode every distribution with pairwise MN**.\n",
    "\n",
    "How do we increase the expressive power of the MN?\n",
    "\n",
    "- Consider Markov networks which are not pairwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 General Gibbs distributions\n",
    "\n",
    "Following the discussion above, we would like to generate networks using general factors, i.e. factors whose scope may contain 2 or more RVs:\n",
    "\n",
    "$$\\phi_i(\\bar{D}_i),$$\n",
    "\n",
    "where $|\\bar{D}_i|$ can be $1$, $2$, $3$ or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Definition*. A distribution $P_{\\Phi}$ is a **Gibbs distribution** parameterized by a set of factors $\\Phi = \\{\\phi_1(\\bar{D}_1), \\dots, \\phi_k(\\bar{D}_k)\\}$ if it is defined as:\n",
    ">\n",
    "> $$P_{\\Phi}(X_1, \\dots, X_n) = \\frac{1}{Z} \\tilde{P}_{\\Phi}(X_1, \\dots, X_n),$$\n",
    ">\n",
    "> where\n",
    "> \n",
    "> $$\\tilde{P}_{\\Phi}(X_1, \\dots, X_n) = \\prod_{i=1}^k \\phi_i(\\bar{D}_i),$$\n",
    ">\n",
    "> is an unnormalized measure and\n",
    ">\n",
    "> $$Z = \\sum_{X_1,\\dots,X_n} \\tilde{P}_{\\Phi}(X_1, \\dots, X_n)$$\n",
    ">\n",
    "> is a normalizing constant called the **partition function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in principle, a Gibbs distribution is not related to an undirected graph. However, it indeed induces a special undirected graph as follows:\n",
    "\n",
    "- If we have a set of factors $\\Phi = \\{\\phi_1(\\bar{D}_1), \\dots, \\phi_k(\\bar{D}_k)\\}$ corresponding to a Gibbs distribution, \n",
    "- the induced Markov network $\\mathcal{H}_{\\Phi}$ has an edge $X_i - X_j$ whenever there exists a factor $\\phi_i\\in\\Phi$ such that $X_i, X_j \\in \\mathrm{scope}[\\phi_i]$.\n",
    "\n",
    "**Markov Networks (MN) are also known as Markov Random Fields (MRF)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example.** We have three factors\n",
    "\n",
    "$$\\phi_1(A,B,C)$$\n",
    "\n",
    "$$\\phi_2(C, D)$$\n",
    "\n",
    "$$\\phi_3(D)$$\n",
    "\n",
    "What is the induced MN structure of these three factors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First solve, then show (InducedMN)\n",
    "Image(\"figures/InducedMN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, in turn induces the concept of factorization of a distribution over a MN:\n",
    "\n",
    "> *Definition.* We say that a distribution $P$ factorizes over $\\mathcal{H}$ if there exists $\\Phi = \\{\\phi_1(\\bar{D}_1), \\dots, \\phi_k(\\bar{D}_k)\\}$ such that $P=P_{\\Phi}$.\n",
    ">\n",
    "> Note that $\\mathcal{H}=\\mathcal{H}_{\\Phi}$ is the graph induced by $\\Phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to note is that the same MN structure $\\mathcal{H}$ can be induced by different sets of factors.\n",
    "\n",
    "**Example**\n",
    "\n",
    "- Which MN structure induces the factors $\\phi_1(A, B, D)$ and $\\phi_2(B, C, D)$?\n",
    "- Which MN structure induces the factors $\\phi_1(A, B)$, $\\phi_2(B, C)$, $\\phi_3(C,D)$, $\\phi_4(A,D)$, and $\\phi_5(B,D)$?\n",
    "- Which MN structure induces the factors $\\phi_1(A, B, D)$, $\\phi_2(B, C)$, and $\\phi_3(C,D)$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First solve, then show (InducedMN2)\n",
    "Image(\"figures/InducedMN2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does the probabilistic influence flows in a MN?**\n",
    "\n",
    "The probabilistic influence can flow along any trail of the graph, regardless the form of the factors.\n",
    "\n",
    "> *Definition.* We say that a trail $X_1 - \\dots - X_k$ is **active given $\\bar{Z}$** if no $X_i$ in the trail is in $\\bar{Z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "1. Which of the following independence assertions hold in the next graph?\n",
    "\n",
    "   - $(A \\perp D | B)$\n",
    "   - <font color=green> $(A \\perp D | C)$ </font>\n",
    "   - <font color=green>$(B \\perp D | C)$ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/InducedMN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Which of the following independence assertions hold in the next graph?\n",
    "\n",
    "   - $(A \\perp D | B)$\n",
    "   - $(A \\perp D | B, C)$\n",
    "   - $(A \\perp C | B)$\n",
    "   - <font color=green>$(A \\perp C | B, D)$</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/InducedMN2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, how do we define a MN in `pgmpy`?\n",
    "\n",
    "- We use the couples example MN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/MNFirst.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pgmpy.models.MarkovModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MN skeleton (edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add factors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which independencies do the model encode?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conditional random fields\n",
    "\n",
    "A popular application of MNs is classification. For this application, however, one does not try to model the joint distribution $P(Y, X_1, \\dots, X_n)$ but the conditional distribution $P(Y|X_1,\\dots,X_n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why not only use a BN?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/NaiveBayes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the above model assumes that:\n",
    "\n",
    "$$(X_i \\perp X_j | C), \\quad \\forall i,j\\in\\{1,\\dots,n\\}, \\quad i\\neq j$$\n",
    "\n",
    "What if $X_i$ and $X_j$ are very correlated?\n",
    "\n",
    "- For example, you would like to perform a segmentation classifier over certain customers population for a marketing campaign. Between the variables you have for describing the customers, there are the $J$(ob) and the $I$(ncome). Do you really have that $(J \\perp I | C)$?\n",
    "\n",
    "Naive Bayes model often leads to incorrect independence assumptions.\n",
    "\n",
    "You may add edges to capture these correlations, but in that case the model will become dense (in both connections and parameterization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional Random Fields (CRF) representation**\n",
    "\n",
    "Instead of modeling the joint distribution $P(\\bar{X}, \\bar{Y})$, we model the conditional distribution $P(\\bar{Y}|\\bar{X})$.\n",
    "\n",
    "- In this scheme, we don't care about the independencies/dependencies in $\\bar{X}$.\n",
    "\n",
    "Formally,\n",
    "\n",
    "> *Definition.* A **conditional random field (CRF)** is an undirected graph $\\mathcal{H}$ whose nodes correspond to $\\bar{X}\\cup\\bar{Y}$, parameterized by a set of factors $\\Phi = \\{\\phi_1(\\bar{D}_1), \\dots, \\phi_k(\\bar{D}_k)\\}$, such that $\\bar{D}_i\\not\\subset\\bar{X}$. This network encodes a conditional probability distribution as follows\n",
    ">\n",
    "> $$P(\\bar{Y}|\\bar{X}) = \\frac{1}{Z(\\bar{X})} \\tilde{P}(\\bar{Y}, \\bar{X}),$$\n",
    ">\n",
    "> \n",
    "> $$\\tilde{P}(\\bar{Y}, \\bar{X}) = \\prod_{i=1}^k \\phi_i(\\bar{D}_i),$$\n",
    ">\n",
    ">\n",
    "> $$Z(\\bar{X}) = \\sum_{\\bar{Y}} \\tilde{P}(\\bar{Y}, \\bar{X}).$$\n",
    ">\n",
    "> In the graph $\\mathcal{H}$, two variables are connected whenever they appear together in the scope of some factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "1. The differences with respect to the Gibbs distribution are subtle. It is only that the partition function is now $Z(\\bar{X}) = \\sum_{\\bar{Y}} \\tilde{P}(\\bar{Y}, \\bar{X}).$\n",
    "\n",
    "2. Unlike the definition of the naive Bayes network, the graph of a CRF may still contain edges between variables in $\\bar{X}$. However, these edges do not encode the structure of a distribution over $\\bar{X}$, since the network does not encode such distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression as CRFs**\n",
    "\n",
    "Consider a CRF:\n",
    "\n",
    "- Over the binary RVs $\\bar{X}=\\{X_1, \\dots, X_n\\}$ and $\\bar{Y} = \\{Y\\}$.\n",
    "\n",
    "- There are pairwise edges between $Y$ and each $X_i$.\n",
    "\n",
    "- The factors are defined as:\n",
    "  \n",
    "  $$\\phi_i(Y, X_i)=\\exp(w_i \\boldsymbol{1}\\{X_i=1, Y=1\\}),$$\n",
    "  \n",
    "  where $w_i\\in\\mathbb{R}$ and $\\boldsymbol{1}$ stands for the indicator function.\n",
    "  \n",
    "- Moreover, there is a single-node factor $\\phi_0(Y)=\\exp(w_0 \\boldsymbol{1}\\{Y=1\\})$.\n",
    "  \n",
    "As a part of the homework, you will show that the conditional probability distribution this CRF encodes corresponds to the logistic regression distribution:\n",
    "\n",
    "$$P(Y=1 | \\bar{x}) = \\frac{\\exp\\left(w_0 + \\sum_{i=1}^{n} w_i x_i\\right)}{1 + \\exp\\left(w_0 + \\sum_{i=1}^{n} w_i x_i\\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/LogisticCRF.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Independencies in MNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bayesian networks, we saw that there is an equivalence between the notions of independence and factorization of the distribution.\n",
    "\n",
    "It happens that this property also holds for Markov networks, under some additional requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization $\\Rightarrow$ Independence\n",
    "\n",
    "> *Theorem.* If $P$ factorizes over $\\mathcal{H}$, then $\\mathcal{H}$ is an I-map for $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independence $\\Rightarrow$ Factorization\n",
    "\n",
    "> *Theorem.* **Let $P$ be a positive distribution**. If $\\mathcal{H}$ is an I-map for $P$, then $P$ factorizes over $\\mathcal{H}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 3. Local structure in MNs\n",
    "\n",
    "We already have seen in the last session some local structures for Bayesian networks. Now, we will se some structures for MN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Log-linear models\n",
    "\n",
    "We often use the tabular representation of the factors due to its simplicity and ease of interpretation.\n",
    "\n",
    "On the other hand, we have already discussed that this tables lead, in many cases, to intractable representations.\n",
    "\n",
    "An important class of representation is achieved parameterizing factors in the log-space.\n",
    "\n",
    "To be more precise, we can rewrite a factor $\\phi(\\bar{D})$ as:\n",
    "\n",
    "$$\\phi(\\bar{D}) = \\exp(-\\epsilon(\\bar{D})),$$\n",
    "\n",
    "where $\\epsilon(\\bar{D}) = -\\ln(\\phi(\\bar{D}))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our original representation of the unnormalized measure is:\n",
    "\n",
    "$$\\tilde{P}(X_1,\\dots,X_n) = \\prod_{i}\\phi_i(\\bar{D}_i).$$\n",
    "\n",
    "Hence, in the logarithmic representation we have:\n",
    "\n",
    "$$\\tilde{P}(X_1,\\dots,X_n) = \\exp\\left(-\\sum_{i}\\epsilon_i(\\bar{D}_i)\\right).$$\n",
    "\n",
    "- The exponential function guarantees that $\\tilde{P}$ is positive no matter the signum of $\\epsilon_i$.\n",
    "- Thus, the terms $\\epsilon_i$ are similar to factors, but without the nonnegativity requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the $\\epsilon_i$ terms are decomposed as $\\epsilon_i(\\bar{D}_i) = w_i f_i(\\bar{D}_i)$, where $w_i \\in \\mathbb{R}$ and $f_i$ are called features.\n",
    "\n",
    "> *Definition.* A **feature** $f(\\bar{D})$ is a function $f: \\mathrm{Val}(\\bar{D}) \\to \\mathbb{R}$.\n",
    "\n",
    "As mentioned above, a feature is no more than a factor without the nonnegativity requirement.\n",
    "\n",
    "One important feature, as we saw in the CRFs example, is the *indicator feature*, that takes the value $1$ for some values $\\bar{x}\\in\\bar{D}$ and $0$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to define log-linear models:\n",
    "\n",
    "> *Definition.* A distribution $P$ is a **log-linear model** over a Markov network $\\mathcal{H}$ if it is associated with:\n",
    "> \n",
    "> - a set of features $\\mathcal{F} = \\{f_1(\\bar{D}_1), \\dots, f_k(\\bar{D}_k)\\}$, and\n",
    "> - a set of weights $w_1, \\dots, w_k$,\n",
    ">\n",
    "> such that\n",
    ">\n",
    "> $$P(X_1, \\dots, X_n) = \\frac{1}{Z}\\exp\\left(-\\sum_{i=1}^{k} w_i f_i(\\bar{D}_i)\\right).$$\n",
    "\n",
    "Note, in the above definition, that:\n",
    "\n",
    "- Each feature $f_i$ has its own scope $\\bar{D}_i$.\n",
    "- Different features can have the same scope.\n",
    "- Log-linear models provide a compact representation for many distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How general are log-linear models?**\n",
    "\n",
    "Consider, for example the factor $\\phi(X_1, X_2)$\n",
    "\n",
    "| $X_1$   | $X_2$   | $\\phi$   |\n",
    "| ------- | ------- | -------- |\n",
    "| $x_1^0$ | $x_2^0$ | $a_{00}$ |\n",
    "| $x_1^0$ | $x_2^1$ | $a_{01}$ |\n",
    "| $x_1^1$ | $x_2^0$ | $a_{10}$ |\n",
    "| $x_1^1$ | $x_2^1$ | $a_{10}$ |\n",
    "\n",
    "\n",
    "Let the features $f_{12}^{kl}(X_1, X_2)$ be defined by:\n",
    "\n",
    "$$f_{12}^{kl} (X_1, X_2) = \\boldsymbol{1}\\{X_1=x_1^k, X_2=x_2^k\\}, \\quad \\mathrm{ for } \\quad k,l=0,1.$$\n",
    "\n",
    "Thus, the above tabular factor can be represented as:\n",
    "\n",
    "$$\\phi(X_1,X_2) = \\exp\\left(-\\sum_{k,l\\in\\{0,1\\}} w_{kl}f_{12}^{kl} (X_1, X_2)\\right),$$\n",
    "\n",
    "with $w_{kl} = -\\ln(a_{kl}).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the above procedure, we can express every tabular factor (every discrete distribution) in terms of a log-linear model.\n",
    "\n",
    "In this sense, we say that log-linear models are a **general representation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the advantage, then?**\n",
    "\n",
    "If both representations are equivalent, then there seems to be no advantage of selecting one or another.\n",
    "\n",
    "We already discussed sharing between and inside models in the context of Bayesian networks. The advantage of log-linear models is that they allow us to represent these parameter similarities much mode directly.\n",
    "\n",
    "This is very important, because in undirected models, parameters are much more difficult to elicit.\n",
    "\n",
    "For example, let's consider **Ising models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "- Ising models are one of the earliest types of MRFs. It's a pairwise MN.\n",
    "- They arose in statistical physics as a model for the energy of a physical system involving a system of interacting atoms.\n",
    "- In these systems, each atom is associated with a binary RV $X_i$ with $\\mathrm{Val}(X_i)\\{-1,+1\\}$, whose value defines the direction of the atom's spin.\n",
    "- The energy function associated with the edges is given by the symmetric function:\n",
    "\n",
    "  $$\\epsilon_{i,j}(x_i,x_j) = w_{i,j}x_ix_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The total energy is:\n",
    "  \n",
    "  $$E(x_1,\\dots,x_n) = \\sum_{i<j} w_{i,j}x_ix_j + \\sum_i u_ix_i,$$\n",
    "  \n",
    "  where the terms $u_ix_i$ account for individual node potentials.\n",
    "  \n",
    "- When $w_{i,j}>0$ the model prefers spins of the atoms $i$ and $j$ aligned. The interaction is called ferromagnetic.\n",
    "\n",
    "- When $w_{i,j}<0$ the interaction is antiferromagnetic.\n",
    "\n",
    "- This model can be used to answer questions about the probability of atom's spin directions, usually as the number of atoms tends to infinity.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Ising model, described by the distribution\n",
    "\n",
    "$$P(X_1,...,X_n) = \\frac{1}{Z}\\exp\\left(-E(X_1,\\dots,X_n)\\right)$$\n",
    "\n",
    "it is common to assume the same weight for every adjacent pair\n",
    "\n",
    "$$w_{i,j}=w, \\quad \\forall i,j.$$\n",
    "\n",
    "Hence we would only need $n+1$ parameters to describe this distribution, instead of $2^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Announces\n",
    "\n",
    "## 1. Quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
