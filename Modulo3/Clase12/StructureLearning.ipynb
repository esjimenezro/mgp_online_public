{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Learning\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"figures/Intro.png\" width=\"600px\" height=\"600px\" />\n",
    "\n",
    "> In the last two sessions we studied how to estimate the parameters of both Bayesian Networks and Markov Networks. We made a strong assumption that we know the network structure in advance.\n",
    ">\n",
    "> In this session we will takcle the task of learning in situations when we don't know the structure of the Bayesian network in advance.\n",
    "\n",
    "> **Objetives:**\n",
    "> - To understand the maximum likelihood score for structure learning in Bayesian Networks.\n",
    "> - To study the BIC score for structure learning in Bayesian Networks.\n",
    "\n",
    "> **References:**\n",
    "> - Probabilistic Graphical Models: Principles and Techniques, By Daphne Koller and Nir Friedman. Ch. 18.\n",
    "> - Mastering Probabilistic Graphical Models Using Python, By Ankur Ankan and Abinash Panda. Ch. 5.\n",
    "> - Probabilistic Graphical Models Specialization, offered through Coursera. Prof. Daphne Koller.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Why would we be interested in learning a structure?\n",
    "\n",
    "- To learn a model for new queries, when the domain expertise is not enough.\n",
    "\n",
    "- For structure discovery, when inferring network structure is a goal itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importance of accurate structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that the true model for some situation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/AccurateStructure.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What happens if an arc is missing?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/AccurateStructure1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model encodes inccorrect independencies.\n",
    "- The correct distribution $P^*$ cannot be learned.\n",
    "- However, it could generalize better :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What happens if an arc is added?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/AccurateStructure2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The model encodes spurious dependencies.\n",
    "- Can correctly learn the correct distribution $P^*$.\n",
    "- More parameters to learn.\n",
    "- In general, leads to worse generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Score-based learning\n",
    "\n",
    "To carry out the structure learning, we define a structure that evaluates how well a structure matches the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/ScoreBasedLearning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we search for the structure that maximizes the score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Likelihood score\n",
    "\n",
    "### 2.1. Introduction\n",
    "\n",
    "The idea under the likelihood score is to find the structure $(\\mathcal{G}, \\theta)$ to maximize the likelihood:\n",
    "\n",
    "$$\\mathrm{score}_L (\\mathcal{G}: \\mathcal{D}) = l((\\hat{\\theta}, \\mathcal{G}): \\mathcal{D}) = \\log \\mathcal{L}((\\hat{\\theta}, \\mathcal{G}): \\mathcal{D})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{\\theta} = \\theta_{MLE}$ is the MLE of the parameters given $\\mathcal{G}$ and $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Consider the two random variables $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, consider the graph structure $\\mathcal{G}_0:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/Example1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood score is:\n",
    "\n",
    "$$\\mathrm{score}_L(\\mathcal{G}_0: \\mathcal{D}) =  \\sum_{d=1}^{M} \\left(\\log\\hat{\\theta}_{x[d]} + \\log\\hat{\\theta}_{x[d]}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider the graph structure $\\mathcal{G}_1:$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/Example2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood score is:\n",
    "\n",
    "$$\\mathrm{score}_L(\\mathcal{G}_1: \\mathcal{D}) = \\sum_{d=1}^{M} \\left(\\log\\hat{\\theta}_{x[d]} + \\log\\hat{\\theta}_{y[d]|x[d]}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the difference between them:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{score}_L(\\mathcal{G}_1: \\mathcal{D}) - \\mathrm{score}_L(\\mathcal{G}_0: \\mathcal{D}) & = \\sum_{d=1}^{M}\\left(\\log\\hat{\\theta}_{y[d]|x[d]} - \\log\\hat{\\theta}_{y[d]}\\right) \\\\\n",
    "& = \\sum_{x,y} M(x, y) \\log\\hat{\\theta}_{y|x} - \\sum_{y} M(y) \\log\\hat{\\theta}_{y} \\\\\n",
    "& = M \\left(\\sum_{x,y} \\hat{P}(x, y) \\log\\hat{P}(x| y) - \\sum_{y} \\hat{P}(y) \\log\\hat{P}(y)\\right),\n",
    "\\end{align}\n",
    "\n",
    "where $\\hat{P}$ is the empirical distribution (frequency counts).\n",
    "\n",
    "Moreover, recall that $\\sum_{x} \\hat{P}(x, y) = \\hat{P}(y)$. Then,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{score}_L(\\mathcal{G}_1: \\mathcal{D}) - \\mathrm{score}_L(\\mathcal{G}_0: \\mathcal{D}) & = M \\left(\\sum_{x,y} \\hat{P}(x, y) \\log\\hat{P}(x| y) - \\sum_{x,y} \\hat{P}(x,y) \\log\\hat{P}(y)\\right) \\\\\n",
    "& = M \\sum_{x,y} \\hat{P}(x, y) \\left(\\log\\hat{P}(x| y) - \\log\\hat{P}(y)\\right) \\\\\n",
    "& = M \\sum_{x,y} \\hat{P}(x, y) \\log\\frac{\\hat{P}(x| y)}{\\hat{P}(y)} \\\\\n",
    "& = M I_{\\hat{P}} (X; Y),\n",
    "\\end{align}\n",
    "\n",
    "where $I_{\\hat{P}} (X; Y)$ is **the mutual information** between the variables $X$ and $Y$ w.r.t. the empirical distribution $\\hat{P}$.\n",
    "\n",
    "Intuitively, the mutual information $I_{\\hat{P}} (X; Y)$ measures how close the variables $X$ and $Y$ are to the independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. General decomposition\n",
    "\n",
    "The above is not only for this simple example. In fact, one can easily show that:\n",
    "\n",
    "$$\\mathrm{score}_L(\\mathcal{G}: \\mathcal{D}) = M \\sum_{i=1}^{n} I_{\\hat{P}}(X_i, \\mathrm{Pa}X_i) - M \\sum_{i=1}^{n} H_{\\hat{P}}(X_i),$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $I_{\\hat{P}}(X; Y) = \\sum_{x,y} P(x, y) \\log \\frac{P(x,y)}{P(x)P(y)}$ is the mutual information.\n",
    "\n",
    "- $H_{\\hat{P}}(X) = - \\sum_{x} P(x) \\log P(x)$ is the entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entropy $H_{\\hat{P}}(X)$ is independent of the graph structure $\\mathcal{G}$.\n",
    "\n",
    "Then, the score is higher if the nodes $X_i$ is correlated with its parents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations:**\n",
    "\n",
    "Following the above, note that the difference of having an arc with not having it is:\n",
    "\n",
    "$$\\mathrm{score}_L(\\mathcal{G}_1: \\mathcal{D}) - \\mathrm{score}_L(\\mathcal{G}_0: \\mathcal{D}) = M I_{\\hat{P}} (X, Y).$$\n",
    "\n",
    "Some comments:\n",
    "- The mutual information is always nonnegative: $I_{\\hat{P}} (X, Y) \\geq 0.$\n",
    "- It equals zero if and onlyt if $X$ and $Y$ are independent (in the empirical distribution $\\hat{P}$).\n",
    "- Even if $X \\perp Y$ in the true distribution $P^*$, almost always $I_{\\hat{P}} (X, Y) > 0$.\n",
    "- Thus, adding edges can't hurt this score, and almost always helps.\n",
    "\n",
    "### The maximum likelihood score is maximized for fully connected networks: overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because of the above, this score is never used (it is not implemented in `pgmpy`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data into a data frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empirical probabilities\n",
    "# P(X)\n",
    "\n",
    "# P(Y)\n",
    "\n",
    "# P(X, Y)\n",
    "\n",
    "# P(X)P(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "# X    Y\n",
    "\n",
    "# X -> Y\n",
    "\n",
    "# X <- Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BIC score\n",
    "\n",
    "How can we avoid overfitting?\n",
    "- We can restrict the hypothesis space:\n",
    "  - Restrict number of parents or parameters.\n",
    "  \n",
    "- We can penalyze the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explicit penalization of the complexity is done by the **Bayesian Information Criterion (BIC) score**:\n",
    "\n",
    "$$\\mathrm{score}_{BIC}(\\mathcal{G}: \\mathcal{D}) = \\mathrm{score}_{L}(\\mathcal{G}: \\mathcal{D}) - \\frac{\\log M}{2} \\mathrm{Dim}[\\mathcal{G}],$$\n",
    "\n",
    "where $\\mathrm{Dim}[\\mathcal{G}]$ is the number of independent parameters implied by the structure $\\mathcal{G}$.\n",
    "\n",
    "This score directly represents the tradeoff between fit to the data and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Asymptotic consistency:**\n",
    "\n",
    "We have that:\n",
    "\n",
    "$$\\mathrm{score}_{BIC}(\\mathcal{G}: \\mathcal{D}) = M \\sum_{i=1}^{n} I_{\\hat{P}}(X_i, \\mathrm{Pa}X_i) - M \\sum_{i=1}^{n} H_{\\hat{P}}(X_i) - \\frac{\\log M}{2} \\mathrm{Dim}[\\mathcal{G}]$$\n",
    "\n",
    "The mutual information grows linearly with $M$ whereas the complexity grows logarithmically with $M$.\n",
    "\n",
    "> As $M\\to\\infty$, more emphasis is given to fit to the data than to model complexity.\n",
    ">\n",
    "> Thus, as $M\\to\\infty$, ($\\hat{P} \\to P^*$) the true structure $\\mathcal{G}^*$ will maximize the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pgmpy.estimators.BicScore, pgmpy.estimators.ExhaustiveSearch, pgmpy.models.BayesianModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BicScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "# X    Y\n",
    "\n",
    "# X -> Y\n",
    "\n",
    "# X <- Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores\n",
    "# X    Y\n",
    "\n",
    "# X -> Y\n",
    "\n",
    "# X <- Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search space of BNs is exponential in the number of variables and the BIC scoring function allow for local maxima. \n",
    "\n",
    "The first property makes exhaustive search intractable for all but very small networks, the second prohibits efficient local optimization algorithms to always find the optimal structure. \n",
    "\n",
    "Thus, identifiying the ideal structure is often not tractable. Despite these bad news, heuristic search strategies often yields good results.\n",
    "\n",
    "If only few nodes are involved (read: less than 5), ExhaustiveSearch can be used to compute the score for every DAG and returns the best-scoring one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exhaustive search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all scores and edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If more nodes are involved, one needs to switch to heuristic search. \n",
    "\n",
    "HillClimbSearch implements a greedy local search that starts from the DAG start (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pgmpy.estimators.HillClimbSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some data with dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hill Climb search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search correctly identifies the independencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended lecture:\n",
    "\n",
    "## Section 18.3.2 Bayesian score (page 794)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
