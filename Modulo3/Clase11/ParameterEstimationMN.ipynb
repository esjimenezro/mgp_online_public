{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation in Markov Networks\n",
    "\n",
    "<img style=\"float: right; margin: 0px 0px 15px 15px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d5/Hey_Machine_Learning_Logo.png\" width=\"400px\" height=\"400px\" />\n",
    "\n",
    "> In the last session we saw how to estimate the parameters of a Bayesian Network. In particular, we saw that the maximum likelihood estimator can be calculated in closed form, simply as the frequentist interpretation of probability.\n",
    ">\n",
    "> In this session we will see how to estimate the parameters of a Markov Network. We will see that tha partition function will make it difficult to obtain a closed form solution of the optimal parameters, and then, we will have to use numerical methods to find them.\n",
    ">\n",
    "> Additionally, we will examine the MAP estimation of the parameters and its relation with including regularization terms in the optimization.\n",
    "\n",
    "> **Objetives:**\n",
    "> - To study the maximum likelihood parameter estimation problem for Markov Networks.\n",
    "> - To study the maximum a posteriori parameter estimation problem for Markov Networks.\n",
    "\n",
    "> **References:**\n",
    "> - Probabilistic Graphical Models: Principles and Techniques, By Daphne Koller and Nir Friedman. Ch. 20.\n",
    "> - Mastering Probabilistic Graphical Models Using Python, By Ankur Ankan and Abinash Panda. Ch. 6.\n",
    "> - Probabilistic Graphical Models Specialization, offered through Coursera. Prof. Daphne Koller.\n",
    "\n",
    "\n",
    "<p style=\"text-align:right;\"> Imagen recuperada de: https://upload.wikimedia.org/wikipedia/commons/d/d5/Hey_Machine_Learning_Logo.png.</p>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Maximum likelihood parameter estimation in log-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Log-likelihood function: lack of separability\n",
    "\n",
    "A key property of the (log-)likelihood function for BNs was that it can be decomposed as the (sum) product of local likelihoods. Then, to maximize the whole function, one could maximize each local function separately.\n",
    "\n",
    "We will see that this principle does not hold for MNs because of the partition function $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "Consider the pairwise MN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"figures/pairwiseMN.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume, for instance, that we know:\n",
    "\n",
    "|  $A$  |  $B$  |  $\\phi_1$  |\n",
    "| ----- | ----- | ---------- |\n",
    "| $a^0$ | $b^0$ |  $1$       |\n",
    "| $a^0$ | $b^1$ |  $1$       |\n",
    "| $a^1$ | $b^0$ |  $1$       |\n",
    "| $a^1$ | $b^1$ | $\\theta_1$ |\n",
    "\n",
    "|  $B$  |  $C$  |  $\\phi_1$  |\n",
    "| ----- | ----- | ---------- |\n",
    "| $b^0$ | $c^0$ |  $1$       |\n",
    "| $b^0$ | $c^1$ | $\\theta_2$ |\n",
    "| $b^1$ | $c^0$ |  $1$       |\n",
    "| $b^1$ | $c^1$ |  $1$       |\n",
    "\n",
    "We want to estimate $\\bar{\\theta} = [\\theta_1, \\theta_2]$ with the IID data\n",
    "\n",
    "$$\\mathcal{D}=\\{(a[1], b[1], c[1]), \\dots, (a[M], b[M], c[M])\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that \n",
    "\n",
    "$$P_{\\Phi}(A,B,C) =% \\frac{1}{Z} \\phi_1(A,B) \\phi_2(B,C),$$\n",
    "\n",
    "with $Z = \\sum_{A,B,C} \\phi_1(A,B) \\phi_2(B,C)$.\n",
    "\n",
    "Thus, the log-likelihood function is (<font color=red> See in the whiteboard</font>):\n",
    "\n",
    "$$l(\\bar{\\theta}: \\mathcal{D}) =% \\sum_{d=1}^{M} \\left(\\log \\phi_1(a[d], b[d]) + \\log \\phi_2(b[d], c[d]) - \\log Z(\\bar{\\theta})\\right),$$\n",
    "\n",
    "with $Z(\\bar{\\theta}) = 4 + 2\\theta_1 + 2 \\theta_2.$\n",
    "\n",
    "Assuming that $M(a^1, b^1)$ and $M(b^0, c^1)$ are the number of times that the joint assignments $a^1, b^1$ and $b^0, c^1$ appear in $\\mathcal{D}$, respectively, we have:\n",
    "\n",
    "$$l(\\bar{\\theta}: \\mathcal{D}) =% M(a^1, b^1) \\log \\theta_1 + M(b^0, c^1) \\log \\theta_2 - M \\log(4 + 2\\theta_1 + 2 \\theta_2).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partition function $Z(\\bar{\\theta}) =% 4 + 2\\theta_1 + 2 \\theta_2$ **couples the parameters**:\n",
    "- It is not possible to decompose the likelihood.\n",
    "- We cannot obtain a closed form estimation for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Log-likelihood for log-linear models\n",
    "\n",
    "Recall that the log-linear models are a general representation. Given a set of features $\\mathcal{F}=\\{f_i(\\bar{D}_i)\\}_{i=1}^{k}$, where $f_i(\\bar{D}_i)$ is a feature function defined over the variables $\\bar{D}_i$, we have that the joint distribution for the log-linear model is:\n",
    "\n",
    "$$P(X_1,\\dots,X_n:\\bar{\\theta}) = \\frac{1}{Z(\\bar{\\theta})} \\exp\\left\\{ \\sum_{i=1}^{k}\\theta_i f_i (\\bar{D}_i)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function is:\n",
    "\n",
    "\\begin{align}\n",
    "l(\\bar{\\theta}:\\mathcal{D}) & = \\sum_{d=1}^{M}\\left(\\sum_{i=1}^{k}\\theta_i f_i (\\bar{x}[d]) - \\log Z(\\bar{\\theta})\\right) \\\\\n",
    "                            & = \\sum_{i=1}^{k}\\theta_i \\sum_{d=1}^{M} f_i (\\bar{x}[d]) - M\\log Z(\\bar{\\theta}),\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "\n",
    "$$\\log Z(\\bar{\\theta}) = \\log \\left(\\sum_{\\bar{X}}\\exp\\left\\{ \\sum_{i=1}^{k}\\theta_i f_i (\\bar{D}_i)\\right\\}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's prove the following results (<font color=red>in the whiteboard</font>):\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i} \\log Z(\\bar{\\theta})= E_{\\theta}[f_i]$$\n",
    "\n",
    "$$\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log Z(\\bar{\\theta})= \\mathrm{cov}_{\\theta}[f_i,f_j]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the Hessian (second derivatives' matrix) of log-partition function $\\log Z(\\bar{\\theta})$ is:\n",
    "\n",
    "$$\\frac{\\partial^2}{\\partial \\theta_i \\partial \\theta_j} \\log Z(\\bar{\\theta})= \\mathrm{cov}_{\\theta}[f_i,f_j]$$\n",
    "\n",
    "the covariance matrix of the features - **semi-positive definite**. In this sense the log-partition is:\n",
    "\n",
    "1. Concave\n",
    "\n",
    "2. Convex\n",
    "\n",
    "3. None of the above\n",
    "\n",
    "Then, $-M \\log Z(\\bar{\\theta})$ is __."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the function $\\sum_{i=1}^{k}\\theta_i \\sum_{d=1}^{M} f_i (\\bar{x}[d])$ is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the log-likelihood function is concave:\n",
    "\n",
    "- No local maxima.\n",
    "- Good theoretical guarantees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maximum likelihood estimation\n",
    "\n",
    "Given the above, we can divide the log-likelihood by the number of samples $M$ and the resulting function would still be concave:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{M} l(\\bar{\\theta}:\\mathcal{D}) & = \\sum_{i=1}^{k}\\theta_i \\frac{1}{M}\\sum_{d=1}^{M} f_i (\\bar{x}[d]) - \\log Z(\\bar{\\theta}) \\\\\n",
    "                                        & = \\sum_{i=1}^{k}\\theta_i E_{\\mathcal{D}}[f_i] - \\log Z(\\bar{\\theta})\n",
    "\\end{align}\n",
    "\n",
    "where $E_{\\mathcal{D}}[f_i] = \\frac{1}{M}\\sum_{d=1}^{M} f_i (\\bar{x}[d])$ is the empirical expectation of the feature $f_i$ in the data $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the gradient of the log-likelihood by the number of samples $M$ is:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i} \\frac{1}{M} l(\\bar{\\theta}:\\mathcal{D}) = E_{\\mathcal{D}}[f_i] - E_{\\theta}[f_i]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Theorem*. Given a set of features $\\mathcal{F}$, $\\hat{\\theta}$ is the MLE if and only if\n",
    ">\n",
    "> $$E_{\\mathcal{D}}[f_i] = E_{\\hat{\\theta}}[f_i]$$\n",
    ">\n",
    "> for all $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, **how do we compute the MLE parameters?**\n",
    "\n",
    "We can use numerical methods. In particular, the first order gradient ascent will do the job\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i} \\frac{1}{M} l(\\bar{\\theta}:\\mathcal{D}) = E_{\\mathcal{D}}[f_i] - E_{\\theta}[f_i].$$\n",
    "\n",
    "For the gradient, we need the expectations of the features:\n",
    "\n",
    "- In data.\n",
    "- Relative to current model: in this step we need to perform inference at each gradient step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `pgmpy` MarkovModel object do not have method fit. We'll do it ourselves to illustrate the above.\n",
    "\n",
    "**Example:** See in the whiteboard the log-linear model of A-B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy.optimize.fmin_cg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some random data for A - B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap data in a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain empirical expectation of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MAP estimation for MNs\n",
    "\n",
    "As for BNs, MLE for MNs is very susceptible to overfitting of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian parameter prior\n",
    "\n",
    "Often, the zero-mean univariate Gaussian (assuming independence of parameters) is used:\n",
    "\n",
    "$$P(\\bar{\\theta}) = \\prod_{i=1}^{k} \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left\\{-\\frac{\\theta_i^2}{2\\sigma^2}\\right\\}$$\n",
    "\n",
    "- $\\sigma^2$ can be interpreted as the confidence that we have for the parameters not being close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian parameter prior\n",
    "\n",
    "Another commonly used prior is the Laplace distribution:\n",
    "\n",
    "$$P(\\bar{\\theta}) = \\prod_{i=1}^{k} \\frac{1}{2\\beta} \\exp\\left\\{-\\frac{|\\theta_i|}{\\beta}\\right\\}$$\n",
    "\n",
    "- $\\beta$ can be interpreted as the confidence that we have for the parameters not being close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP estimation and regularization\n",
    "\n",
    "What happens when we maximize the a posteriori distribution?\n",
    "\n",
    "\\begin{align}\n",
    "\\arg \\max_{\\theta} P(\\mathcal{D}, \\bar{\\theta}) & = \\arg \\max_{\\theta} P(\\mathcal{D}| \\bar{\\theta}) P(\\bar{\\theta}) \\\\\n",
    "                                                & = \\arg \\max_{\\theta} \\left(l(\\bar{\\theta}:\\mathcal{D}) + \\log P(\\bar{\\theta})\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $P$ is Gaussian: $-log P(\\bar{\\theta}) \\equiv L_2$ (dense)\n",
    "- If $P$ is Laplacian: $-log P(\\bar{\\theta}) \\equiv L_1$ (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP objective function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP objective function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
